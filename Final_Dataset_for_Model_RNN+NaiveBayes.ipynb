{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ECG-ViEW - Final "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pylab as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.framework import ops\n",
    "print(tf.__version__)\n",
    "\n",
    "#Visualization Libraries\n",
    "import seaborn as sns\n",
    "\n",
    "# Size of matplotlib histogram bins\n",
    "bin_size = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RR</th>\n",
       "      <th>PR</th>\n",
       "      <th>QRS</th>\n",
       "      <th>QT</th>\n",
       "      <th>QTc</th>\n",
       "      <th>P_wave_axis</th>\n",
       "      <th>QRS_axis</th>\n",
       "      <th>T_wave_axis</th>\n",
       "      <th>ACCI</th>\n",
       "      <th>sex</th>\n",
       "      <th>Birthyeargroup</th>\n",
       "      <th>MI</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>769</td>\n",
       "      <td>204</td>\n",
       "      <td>116</td>\n",
       "      <td>364</td>\n",
       "      <td>414</td>\n",
       "      <td>56</td>\n",
       "      <td>-81</td>\n",
       "      <td>43</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1000</td>\n",
       "      <td>220</td>\n",
       "      <td>124</td>\n",
       "      <td>428</td>\n",
       "      <td>428</td>\n",
       "      <td>59</td>\n",
       "      <td>-74</td>\n",
       "      <td>33</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>769</td>\n",
       "      <td>198</td>\n",
       "      <td>116</td>\n",
       "      <td>398</td>\n",
       "      <td>453</td>\n",
       "      <td>51</td>\n",
       "      <td>268</td>\n",
       "      <td>36</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1000</td>\n",
       "      <td>220</td>\n",
       "      <td>124</td>\n",
       "      <td>416</td>\n",
       "      <td>416</td>\n",
       "      <td>51</td>\n",
       "      <td>-58</td>\n",
       "      <td>23</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>968</td>\n",
       "      <td>228</td>\n",
       "      <td>122</td>\n",
       "      <td>436</td>\n",
       "      <td>442</td>\n",
       "      <td>54</td>\n",
       "      <td>-59</td>\n",
       "      <td>34</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     RR   PR  QRS   QT  QTc  P_wave_axis  QRS_axis  T_wave_axis  ACCI  sex  \\\n",
       "0   769  204  116  364  414           56       -81           43     8    1   \n",
       "1  1000  220  124  428  428           59       -74           33     8    1   \n",
       "2   769  198  116  398  453           51       268           36     8    1   \n",
       "3  1000  220  124  416  416           51       -58           23     8    1   \n",
       "4   968  228  122  436  442           54       -59           34     9    1   \n",
       "\n",
       "   Birthyeargroup  MI  \n",
       "0              12   0  \n",
       "1              12   0  \n",
       "2              12   0  \n",
       "3              12   0  \n",
       "4              12   0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('Final_Dataset.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(98903, 12)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    97863\n",
       "1     1040\n",
       "Name: MI, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['MI'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(98903, 12)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = df.values\n",
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(98903, 11) (98903,)\n",
      "(79122, 11) (79122,)\n",
      "(19781, 11) (19781,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "# Training data features\n",
    "train_features = train_data[:, 0:11]\n",
    "\n",
    "# 'MI' column values\n",
    "train_target = train_data[:, 11]\n",
    "\n",
    "# defining the scaler\n",
    "robust_scaler = RobustScaler().fit(train_data)\n",
    "\n",
    "# scaling test data using minmax scaling\n",
    "train_data_robust = robust_scaler.transform(train_data)\n",
    "\n",
    "# training data features\n",
    "train_features_robust= train_data_robust[:, 0:11]\n",
    "\n",
    "# 'MI' column values\n",
    "train_target_robust = train_data_robust[:, 11]\n",
    "\n",
    "# Split 80-20 train vs test data\n",
    "train_x, test_x, train_y, test_y = train_test_split(train_features, \n",
    "                                                    train_target, \n",
    "                                                    test_size=0.20, \n",
    "                                                    random_state=0,\n",
    "                                                    shuffle=True)\n",
    "\n",
    "print (train_features.shape, train_target.shape)\n",
    "print (train_x.shape, train_y.shape)\n",
    "print (test_x.shape, test_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 78282, 1: 840}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique, counts = np.unique(train_y, return_counts=True)\n",
    "dict(zip(unique, counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 19581, 1: 200}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique, counts = np.unique(test_y, return_counts=True)\n",
    "dict(zip(unique, counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import imblearn\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline\n",
    "# minority oversampling using SMOTE\n",
    "over = SMOTE(sampling_strategy=0.25) # get 25% of the data from minority\n",
    "under = RandomUnderSampler(sampling_strategy=0.5) # make majority only 50% more than the minority in final dataset\n",
    "steps = [('o', over), ('u', under)]\n",
    "pipeline = Pipeline(steps=steps)\n",
    "# transform the dataset\n",
    "x_re, y_re = pipeline.fit_resample(train_features_robust, train_target_robust)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0.0: 48930, 1.0: 24465}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique, counts = np.unique(y_re, return_counts=True)\n",
    "dict(zip(unique, counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(98903, 11) (98903,)\n",
      "(58716, 11) (58716,)\n",
      "(14679, 11) (14679,)\n"
     ]
    }
   ],
   "source": [
    "# Split 80-20 train vs test data\n",
    "train_x, test_x, train_y, test_y = train_test_split(x_re, \n",
    "                                                    y_re, \n",
    "                                                    test_size=0.20, \n",
    "                                                    random_state=0,\n",
    "                                                    shuffle=True)\n",
    "\n",
    "print (train_features.shape, train_target.shape)\n",
    "print (train_x.shape, train_y.shape)\n",
    "print (test_x.shape, test_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0.0: 39114, 1.0: 19602}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the new distribution in the training and testing sets\n",
    "unique, counts = np.unique(train_y, return_counts=True)\n",
    "dict(zip(unique, counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0.0: 9816, 1.0: 4863}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique, counts = np.unique(test_y, return_counts=True)\n",
    "dict(zip(unique, counts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing libraries\n",
    "from keras import optimizers, losses, activations, models\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, LearningRateScheduler, ReduceLROnPlateau\n",
    "from keras.layers import Layer, GRU, LSTM, Dense, Input, Dropout, Convolution1D, MaxPool1D, GlobalMaxPool1D, GlobalAveragePooling1D, \\\n",
    "    concatenate\n",
    "from keras.layers import LeakyReLU\n",
    "from keras import regularizers, backend, initializers\n",
    "from keras.models import Sequential\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.metrics import f1_score, accuracy_score, roc_auc_score\n",
    "from sklearn import preprocessing\n",
    "from keras.initializers import Ones, Zeros\n",
    "import keras.backend as K\n",
    "import time\n",
    "import gc\n",
    "\n",
    "class LayerNormalization(Layer):\n",
    "    def __init__(self, eps=1e-6, **kwargs):\n",
    "        self.eps = eps\n",
    "        super(LayerNormalization, self).__init__(**kwargs)\n",
    "    def build(self, input_shape):\n",
    "        self.gamma = self.add_weight(name='gamma', shape=input_shape[-1:],\n",
    "                                     initializer=Ones(), trainable=True)\n",
    "        self.beta = self.add_weight(name='beta', shape=input_shape[-1:],\n",
    "                                    initializer=Zeros(), trainable=True)\n",
    "        super(LayerNormalization, self).build(input_shape)\n",
    "    def call(self, x):\n",
    "        mean = K.mean(x, axis=-1, keepdims=True)\n",
    "        std = K.std(x, axis=-1, keepdims=True)\n",
    "        return self.gamma * (x - mean) / (std + self.eps) + self.beta\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape\n",
    "\n",
    "layer_size1 = 12\n",
    "layer_size2 = 10\n",
    "layer_size3 = 7\n",
    "layer_size4 = 5\n",
    "layer_size5 = 4\n",
    "layer_size6 = 3\n",
    "timesteps = 1 # static data\n",
    "data_dim = 11\n",
    "\n",
    "X_train = np.reshape(train_x, (train_x.shape[0], 1, train_x.shape[1]))\n",
    "X_test = np.reshape(test_x, (test_x.shape[0], 1, test_x.shape[1]))\n",
    "train_y = to_categorical(train_y)\n",
    "\n",
    "#  use_bias=True, bias_initializer=initializers.Constant(-1)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(GRU(layer_size1, return_sequences=True, input_shape=(timesteps, data_dim)))\n",
    "model.add(LayerNormalization())\n",
    "model.add(LeakyReLU(alpha=0.01))\n",
    "model.add(GRU(layer_size2, return_sequences=True))\n",
    "model.add(LayerNormalization())\n",
    "model.add(LeakyReLU(alpha=0.01))\n",
    "model.add(GRU(layer_size3, return_sequences=True))\n",
    "model.add(LayerNormalization())\n",
    "model.add(LeakyReLU(alpha=0.01))\n",
    "model.add(GRU(layer_size4, return_sequences=True))\n",
    "model.add(LayerNormalization())\n",
    "model.add(LeakyReLU(alpha=0.01))\n",
    "model.add(GRU(layer_size5, return_sequences=True))\n",
    "model.add(LayerNormalization())\n",
    "model.add(LeakyReLU(alpha=0.01))\n",
    "model.add(GRU(layer_size6, return_sequences=False))\n",
    "model.add(LayerNormalization())\n",
    "model.add(LeakyReLU(alpha=0.01))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "\n",
    "start = time.time()\n",
    "opt = optimizers.Adam(0.001)\n",
    "model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "\n",
    "# Stop the training when there is no improvement in the validation accuracy for ten consecutive epochs.\n",
    "early = EarlyStopping(monitor='val_accuracy', patience=10, verbose=0)\n",
    "\n",
    "# reduces learning rate when a metric has stopped improving\n",
    "redonplat = ReduceLROnPlateau(monitor=\"val_accuracy\", mode=\"max\", patience=7, verbose=0)\n",
    "\n",
    "# defining the callbacks list to include the above parameters\n",
    "callbacks_list = [early, redonplat]\n",
    "\n",
    "# train the model\n",
    "history = model.fit(X_train, train_y, epochs=1000, verbose=0, callbacks=callbacks_list, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the model\n",
    "pred_test = model.predict(X_test)\n",
    "pred_test = np.argmax(pred_test, axis=-1)\n",
    "\n",
    "# get f1 score of the model & print it. The f1 score considers the precision & recall.\n",
    "f1 = f1_score(test_y, pred_test, average=\"macro\")\n",
    "print(\"Test f1 score : %s \"% f1)\n",
    "\n",
    "# get ROC AUC score of the model & print it\n",
    "roc = roc_auc_score(test_y, pred_test)\n",
    "print(\"Test ROC AUC Score : %s \"% roc)\n",
    "\n",
    "# get the accuracy and print it\n",
    "acc = accuracy_score(test_y, pred_test)\n",
    "print(\"Test accuracy score : %s \"% acc)\n",
    "\n",
    "# Save the model\n",
    "model.save(\"model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot model accuracy evolution\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot model loss evolution\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gaussian Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Gaussian Naive Bayes model\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# Takes one class only, but since they contain duplicate information, remove one class\n",
    "train_y_nb = train_y[:,1]\n",
    "train_y_nb.reshape(-1, 1)\n",
    "test_y_nb = test_y[:,]\n",
    "test_y_nb.reshape(-1, 1)\n",
    "\n",
    "# Create a Gaussian Classifier\n",
    "gnb = GaussianNB()\n",
    "\n",
    "# Train the model using the training sets\n",
    "gnb.fit(train_x, train_y_nb)\n",
    "\n",
    "# Predict the response for test dataset\n",
    "y_pred_gnb = gnb.predict(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Accuracy\n",
    "print(\"Accuracy:\", accuracy_score(test_y_nb, y_pred_gnb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Complement Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Import Complement Naive Bayes model\n",
    "from sklearn.naive_bayes import ComplementNB\n",
    "\n",
    "# Algorithm cannot take negative feature values so min/max them to [0,1]\n",
    "scaler1 = preprocessing.MinMaxScaler()\n",
    "\n",
    "scaler1.fit(train_x)\n",
    "\n",
    "train_x_cnb = scaler1.transform(train_x)\n",
    "\n",
    "scaler2 = preprocessing.MinMaxScaler()\n",
    "\n",
    "scaler2.fit(train_x)\n",
    "\n",
    "test_x_cnb = scaler2.transform(test_x)\n",
    "\n",
    "print(train_x_cnb.shape)\n",
    "\n",
    "# Create the classifier\n",
    "clf = ComplementNB()\n",
    "\n",
    "# Train the model using the training sets\n",
    "clf.fit(train_x_cnb, train_y_nb)\n",
    "\n",
    "# Predict the response for test dataset\n",
    "y_pred_cnb = clf.predict(test_x_cnb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Accuracy\n",
    "print(\"Accuracy:\", accuracy_score(test_y, y_pred_cnb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
